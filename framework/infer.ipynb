{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc330109-960f-4849-8b82-f001a73a9ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "YamlNamespace(batch_size=8, checkpoint_epoch=63, dataset_folder=WindowsPath('C:/Users/Mohammad/Documents/M2A/AppAuto/challenge-ens/data/dataset_UNZIPPED'), seed=42, set='test', xp_dir=WindowsPath('C:/Users/Mohammad/Documents/M2A/AppAuto/challenge-ens/code/experiments/Final'), xp_name='Final', xp_rootdir=WindowsPath('C:/Users/Mohammad/Documents/M2A/AppAuto/challenge-ens/code/experiments'))\n",
      "Instanciate test dataset\n",
      "Predict the vectors over the test dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "631it [00:52, 12.08it/s]                                                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving prediction CSV to file C:\\Users\\Mohammad\\Documents\\M2A\\AppAuto\\challenge-ens\\code\\experiments\\Final\\epoch63_test_predicted.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Predict the targets using a trained model.\n",
    "\"\"\"\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "import yaml\n",
    "import random\n",
    "from tifffile import TiffFile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "\n",
    "from dataset import LandCoverData as LCD\n",
    "from utils import YamlNamespace\n",
    "\n",
    "def numpy_parse_image(image_path):\n",
    "    \"\"\"Load an image as numpy array\n",
    "    Args:\n",
    "        image_path (bytes): path to image\n",
    "    Returns:\n",
    "        numpy.array[uint8]: the image array\n",
    "    \"\"\"\n",
    "    image_path = Path(bytes.decode(image_path))\n",
    "    with TiffFile(image_path) as tifi:\n",
    "        image = tifi.asarray()\n",
    "    return image\n",
    "\n",
    "@tf.function(input_signature=[tf.TensorSpec(None, tf.string)])\n",
    "def parse_image(image_path):\n",
    "    \"\"\"Wraps the parse_image function as a TF function\"\"\"\n",
    "    image, = tf.numpy_function(numpy_parse_image, (image_path,), (tf.uint16,))\n",
    "    image.set_shape([LCD.IMG_SIZE, LCD.IMG_SIZE, LCD.N_CHANNELS])\n",
    "    return image\n",
    "\n",
    "@tf.function\n",
    "def normalize(input_image):\n",
    "    \"\"\"Rescale the pixel values of the images between 0.0 and 1.0\"\"\"\n",
    "    image = tf.cast(input_image, tf.float32) / LCD.TRAIN_PIXELS_MAX\n",
    "    return image\n",
    "\n",
    "@tf.function\n",
    "def load_image_test(input_image):\n",
    "    \"\"\"Normalize test image\"\"\"\n",
    "    image = normalize(input_image)\n",
    "    return image\n",
    "\n",
    "\n",
    "def predict_as_vectors(model, dataset, steps=None):\n",
    "    \"\"\"Perform a forward pass over the dataset and bincount the prediction masks to return class vectors.\n",
    "    Args:\n",
    "        model (tf.keras.Model): model\n",
    "        dataset (tf.data.Dataset): dataset to perform inference on\n",
    "        steps (int, optional): the total number of steps (batches) in the dataset, used for the progress bar\n",
    "    Returns:\n",
    "        (pandas.DataFrame): predicted class distribution vectors for the dataset\n",
    "    \"\"\"\n",
    "    def bincount_along_axis(arr, minlength=None, axis=-1):\n",
    "        \"\"\"Bincounts a tensor along an axis\"\"\"\n",
    "        if minlength is None:\n",
    "            minlength = tf.reduce_max(arr) + 1\n",
    "        mask = tf.equal(arr[..., None], tf.range(minlength, dtype=arr.dtype))\n",
    "        return tf.math.count_nonzero(mask, axis=axis-1 if axis < 0 else axis)\n",
    "\n",
    "    predictions = []\n",
    "    for batch in tqdm(dataset, total=steps):\n",
    "        # predict a raster for each sample in the batch\n",
    "        pred_raster = model.predict_on_batch(batch)\n",
    "\n",
    "        (batch_size, _, _, num_classes) = tuple(pred_raster.shape)\n",
    "        pred_mask = tf.argmax(pred_raster, -1) # (bs, 256, 256)\n",
    "        # bincount for each sample\n",
    "        counts = bincount_along_axis(\n",
    "            tf.reshape(pred_mask, (batch_size, -1)), minlength=num_classes, axis=-1\n",
    "        )\n",
    "        predictions.append(counts / tf.math.reduce_sum(counts, -1, keepdims=True))\n",
    "\n",
    "    predictions = tf.concat(predictions, 0)\n",
    "    return predictions.numpy()\n",
    "\n",
    "\n",
    "def _parse_args():\n",
    "#     parser = argparse.ArgumentParser('Inference script')\n",
    "#     parser.add_argument('--config', '-c', type=str, required=True, help=\"The YAML config file\")\n",
    "\n",
    "#     cli_args = parser.parse_args()\n",
    "#     # parse the config file\n",
    "#     with open(cli_args.config, 'r') as f:\n",
    "    with open('C:/Users/Mohammad/Documents/M2A/AppAuto/challenge-ens/framework/infer_config.yaml', 'r') as f:\n",
    "        config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    config = YamlNamespace(config)\n",
    "    config.dataset_folder = Path(config.dataset_folder).expanduser()\n",
    "    assert config.dataset_folder.is_dir()\n",
    "    config.xp_rootdir = Path(config.xp_rootdir).expanduser()\n",
    "    assert config.xp_rootdir.is_dir()\n",
    "    if config.xp_name == 'last':\n",
    "        # get last xp directory name\n",
    "        config.xp_dir = Path(max(str(d) for d in config.xp_rootdir.iterdir() if d.is_dir()))\n",
    "    else:\n",
    "        config.xp_dir = config.xp_rootdir/config.xp_name\n",
    "    assert config.xp_dir.is_dir()\n",
    "    assert config.set in ('train', 'test', 'val')\n",
    "\n",
    "    return config\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    import multiprocessing\n",
    "\n",
    "    config = _parse_args()\n",
    "    print(f'Config:\\n{config}')\n",
    "    # set random seed for reproducibility\n",
    "    if config.seed is not None:\n",
    "        random.seed(config.seed)\n",
    "        np.random.seed(config.seed)\n",
    "        tf.random.set_seed(config.seed)\n",
    "\n",
    "    N_CPUS = multiprocessing.cpu_count()\n",
    "\n",
    "    print(f\"Instanciate {config.set} dataset\")\n",
    "    if config.set == 'test':\n",
    "        test_files = sorted(config.dataset_folder.glob('test/images/*.tif'))\n",
    "    else:\n",
    "        val_samples_s = pd.read_csv(config.xp_dir/'val_samples.csv', squeeze=True)\n",
    "        val_files = [config.dataset_folder/'train/images/{}.tif'.format(i) for i in val_samples_s]\n",
    "        if config.set == 'train':\n",
    "            test_files = [\n",
    "                f for f in sorted(config.dataset_folder.glob('train/images/*.tif'))\n",
    "                if f not in set(val_files)\n",
    "            ]\n",
    "        else:\n",
    "            test_files = val_files\n",
    "    testset_size = len(test_files)\n",
    "\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices(list(map(str, test_files)))\n",
    "    # assert that samples are loaded in the right order (not shuffled)\n",
    "    # for idx, (f, tensor) in enumerate(zip(test_files, test_dataset)):\n",
    "    #     assert f == str(bytes.decode(tensor.numpy()))\n",
    "\n",
    "    test_dataset = test_dataset.map(parse_image, num_parallel_calls=N_CPUS)\\\n",
    "        .map(load_image_test, num_parallel_calls=N_CPUS)\\\n",
    "        .repeat(1)\\\n",
    "        .batch(config.batch_size)\\\n",
    "        .prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    # Load the trained model saved to disk\n",
    "    model = tf.keras.models.load_model(str(config.xp_dir/f'checkpoints/epoch{config.checkpoint_epoch}'))\n",
    "\n",
    "    print(f\"Predict the vectors over the {config.set} dataset\")\n",
    "    y_pred = predict_as_vectors(model, test_dataset, steps=testset_size // config.batch_size)\n",
    "\n",
    "    # get the samples ids\n",
    "    ids_s = pd.Series([int(f.stem) for f in test_files], name='sample_id', dtype='uint32')\n",
    "    df_y_pred = pd.DataFrame(\n",
    "        y_pred, index=ids_s, columns=LCD.CLASSES\n",
    "    )\n",
    "    out_csv = config.xp_dir/f'epoch{config.checkpoint_epoch}_{config.set}_predicted.csv'\n",
    "    print(f\"Saving prediction CSV to file {str(out_csv)}\")\n",
    "    df_y_pred.to_csv(out_csv, index=True, index_label='sample_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d48c6e3-4945-4912-9c29-c970bc3064cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
